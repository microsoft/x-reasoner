<div align="center">
  <h1>X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains</h1>
  <p>A training recipe that optimizes the reasoning capability of VLMs with SFT and RL on general-domain text-only data.
 </p>
 <a href="https://arxiv.org/abs/2505.03981"><b>Paper Arxiv Link </b>ğŸ‘ï¸</a>
</div>
<br>


## News

- May 6, 2025: Our paper is released on arxiv: [arxiv link](https://arxiv.org/abs/2505.03981)
- (We will release the models soon.)

## Results

![results](readme_files/fig1.png)

Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks

## Method

![method](readme_files/method.png)

Xâ€‘Reasoner is built with a *twoâ€‘stage, textâ€‘only postâ€‘training pipeline* that adds robust reasoning skills to a visionâ€‘language model (VLM) and lets those skills generalize across both modalities and domains.

### Step 1:â€¯Supervised fineâ€‘tuning (SFT) with long chainsâ€‘ofâ€‘thought

* **Starting point**â€ƒWe begin with the instructionâ€‘tuned `Qwenâ€‘2.5â€‘VLâ€‘7Bâ€‘Instruct` checkpoint, which already follows prompts but lacks explicit reasoning ability.
* **Data**â€ƒWe fineâ€‘tune on *OpenThoughtsâ€‘114k*, a 114â€¯kâ€‘example dataset of math, coding, and science questions whose *long* chainâ€‘ofâ€‘thought (CoT) rationales were distilled from the stronger DeepSeekâ€‘R1 model. Link: [open-thoughts](https://github.com/open-thoughts/open-thoughts)
* **Algorithm** We train the model with SFT for 4 epochs.

### Step 2:â€¯Reinforcement learning with verifiable rewards (RLVR)
* **Why RL?**â€ƒSFT yields structured reasoning but can overâ€‘explain and drift; RL trims length and sharpens correctness.
* **Data**â€ƒWe use *Orzâ€‘Mathâ€‘57k*, a curated math dataset for RLVR from [Open Reasonder Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero).
* **Algorithm**â€ƒWe adopt *Groupâ€‘Relative Policy Optimisation (GRPO)*, a PPOâ€‘style update that compares multiple sampled answers per query.  Outcome supervision is applied with *exact answer correctness* (1â€¯/â€¯0).

### Domainâ€‘specific extension: Xâ€‘Reasonerâ€‘Med

To demonstrate that X-Reasoner serves as a strong foundation for domain specialization, we further train it on the MedQA dataset using the same SFTâ€¯â†’â€¯RLVR recipe.
The resulting **Xâ€‘Reasonerâ€‘Med** sets new 7Bâ€‘scale stateâ€‘ofâ€‘theâ€‘art scores on a suite of textual and imageâ€‘based medicalâ€‘reasoning tasks.

### Key design choices

* **Pure textual supervision**â€ƒAll optimisation steps use only text; the frozen vision encoder still benefits from better languageâ€‘side reasoning.
* **General-domain supervision** We leverage general-domain data to promote cross-domain generalization and to initialize domain-specialization. 
* **Long CoTâ€¯+â€¯verifiable RL**â€ƒSFT teaches rich, stepâ€‘byâ€‘step reasoning; RLVR then rewards finalâ€‘answer accuracy, delivering strong, robust gains.
  

## References

The codebase for SFT is a fork [Llama Cookbook](https://github.com/meta-llama/llama-cookbook) (we forked it when it was named `llama-recpies`).
This codebase for RL is a fork of the [veRL](https://github.com/volcengine/verl) project with the support for vision language models.
We thank all the authors for providing such a high-performance SFT and RL training framework.

## Citation

```
@misc{liu2025xreasonergeneralizablereasoningmodalities,
      title={X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains}, 
      author={Qianchu Liu and Sheng Zhang and Guanghui Qin and Timothy Ossowski and Yu Gu and Ying Jin and Sid Kiblawi and Sam Preston and Mu Wei and Paul Vozila and Tristan Naumann and Hoifung Poon},
      year={2025},
      eprint={2505.03981},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.03981}
}
```
